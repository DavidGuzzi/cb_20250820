# Plan de Migraci√≥n: Unificaci√≥n de Fuentes de Datos

> **‚úÖ ESTADO: MIGRACI√ìN COMPLETADA (Octubre 10, 2025)**
> Este documento describe el plan original de migraci√≥n a PostgreSQL.
> **Para el estado actual del sistema, ver `NEXT_STEPS.md`**

---

## üìã Resumen Ejecutivo

**Objetivo**: Migrar de fuentes de datos h√≠bridas (datos simulados para chatbot + datos reales para dashboard) hacia una √∫nica fuente de verdad centralizada.

**Opciones evaluadas**: SQLite (recomendado) vs PostgreSQL

**Impacto**: Consistencia total entre chatbot y dashboard, eliminaci√≥n de duplicaci√≥n de datos, mejora en mantenimiento.

----

## üéØ Situaci√≥n Actual vs Objetivo

### **ANTES (Estado H√≠brido)**
```
Chatbot:
‚îú‚îÄ‚îÄ data_store.py (datos simulados)
‚îú‚îÄ‚îÄ sql_engine.py (SQLite in-memory)
‚îî‚îÄ‚îÄ 8 PDVs + 3 meses ficticios

Dashboard:
‚îú‚îÄ‚îÄ API endpoints separados
‚îú‚îÄ‚îÄ Datos reales de A/B testing
‚îî‚îÄ‚îÄ Sin conexi√≥n con chatbot
```

### **DESPU√âS (Fuente Unificada)**
```
Base de Datos √önica:
‚îú‚îÄ‚îÄ Todos los datos reales
‚îú‚îÄ‚îÄ Chatbot y Dashboard consumen la misma fuente
‚îî‚îÄ‚îÄ Consistencia garantizada
```

---

## üõ†Ô∏è OPCI√ìN 1: Migraci√≥n a SQLite (RECOMENDADA)

### **Ventajas**
- ‚úÖ Ideal para datasets peque√±os/medianos
- ‚úÖ Sin dependencias externas
- ‚úÖ Perfecto para text-to-SQL del LLM
- ‚úÖ Deploy simple (archivo √∫nico)
- ‚úÖ Performance excelente para reads
- ‚úÖ Backup/restore trivial

### **Limitaciones**
- ‚ö†Ô∏è Concurrencia limitada (suficiente para la app actual)
- ‚ö†Ô∏è No escalable a m√∫ltiples instancias sin coordinaci√≥n

---

## üìÖ Plan de Implementaci√≥n SQLite

### **FASE 1: Preparaci√≥n y An√°lisis (1-2 d√≠as)**

#### **1.1 Auditor√≠a de Datos Actuales**
```bash
# Tareas:
- Inventario completo de endpoints dashboard existentes
- Identificar estructura de datos reales vs simulados
- Mapear dependencies entre frontend y backend
- Documentar schemas actuales
```

#### **1.2 Dise√±o del Schema Unificado**
```sql
-- Archivo: backend/database/schema.sql

-- Tabla principal de experimentos A/B
CREATE TABLE experimentos (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source TEXT NOT NULL,
    tipologia TEXT NOT NULL,
    palanca TEXT NOT NULL,
    kpi TEXT NOT NULL,
    variacion_promedio REAL NOT NULL,
    diferencia_vs_control REAL NOT NULL,
    period TEXT,
    test_value REAL,
    control_value REAL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabla de maestros (mapeos)
CREATE TABLE maestros (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tipo TEXT NOT NULL, -- 'tipologia', 'palanca', 'kpi'
    codigo INTEGER NOT NULL,
    nombre TEXT NOT NULL,
    activo BOOLEAN DEFAULT 1,
    UNIQUE(tipo, codigo)
);

-- Tabla de PDVs (unificada)
CREATE TABLE pdvs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    codigo TEXT UNIQUE NOT NULL,
    nombre TEXT NOT NULL,
    ciudad TEXT NOT NULL,
    region TEXT NOT NULL,
    tipo TEXT NOT NULL,
    activo BOOLEAN DEFAULT 1
);

-- Tabla de m√©tricas de revenue (para chatbot)
CREATE TABLE metricas_revenue (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    pdv_codigo TEXT NOT NULL,
    periodo TEXT NOT NULL, -- YYYY-MM format
    revenue REAL NOT NULL,
    visitantes INTEGER NOT NULL,
    conversiones INTEGER NOT NULL,
    tasa_conversion REAL GENERATED ALWAYS AS (
        CASE WHEN visitantes > 0 
        THEN (CAST(conversiones AS REAL) / visitantes) * 100 
        ELSE 0 END
    ) STORED,
    revenue_por_visitante REAL GENERATED ALWAYS AS (
        CASE WHEN visitantes > 0 
        THEN revenue / visitantes 
        ELSE 0 END
    ) STORED,
    FOREIGN KEY (pdv_codigo) REFERENCES pdvs(codigo),
    UNIQUE(pdv_codigo, periodo)
);

-- Vista unificada para el chatbot (equivalente a vista_completa actual)
CREATE VIEW vista_completa AS
SELECT 
    m.*,
    p.nombre as pdv_nombre,
    p.ciudad,
    p.region,
    p.tipo as pdv_tipo
FROM metricas_revenue m
JOIN pdvs p ON m.pdv_codigo = p.codigo
WHERE p.activo = 1;

-- √çndices para performance
CREATE INDEX idx_experimentos_tipologia ON experimentos(tipologia);
CREATE INDEX idx_experimentos_palanca ON experimentos(palanca);
CREATE INDEX idx_experimentos_kpi ON experimentos(kpi);
CREATE INDEX idx_metricas_periodo ON metricas_revenue(periodo);
CREATE INDEX idx_metricas_pdv ON metricas_revenue(pdv_codigo);
```

### **FASE 2: Backend - Nueva Capa de Datos (2-3 d√≠as)**

#### **2.1 Crear Servicio de Base de Datos Unificado**
```python
# Archivo: backend/app/services/database_service.py

import sqlite3
import pandas as pd
from typing import List, Dict, Any, Optional
from contextlib import contextmanager
import logging

class UnifiedDatabaseService:
    """Servicio unificado para acceso a datos - SQLite"""
    
    def __init__(self, db_path: str = 'data/unified.db'):
        self.db_path = db_path
        self.logger = logging.getLogger('chatbot_app.database')
        self._initialize_database()
    
    def _initialize_database(self):
        """Inicializa la base de datos con el schema"""
        with sqlite3.connect(self.db_path) as conn:
            # Ejecutar schema.sql
            with open('database/schema.sql', 'r') as f:
                conn.executescript(f.read())
    
    @contextmanager
    def get_connection(self):
        """Context manager para conexiones SQLite"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row  # Para acceso por nombre de columna
        try:
            yield conn
        finally:
            conn.close()
    
    # M√©todos para Dashboard
    def get_dashboard_results(self, tipologia: Optional[str] = None) -> List[Dict]:
        """Obtiene datos para la tabla del dashboard"""
        with self.get_connection() as conn:
            query = """
                SELECT source, kpi, palanca, variacion_promedio, diferencia_vs_control
                FROM experimentos 
                WHERE ($1 IS NULL OR tipologia = $1)
                ORDER BY source, palanca, kpi
            """
            return [dict(row) for row in conn.execute(query, [tipologia])]
    
    def get_evolution_data(self, palanca_id: int, kpi_id: int, tipologia: Optional[str] = None) -> List[Dict]:
        """Obtiene datos para el timeline chart"""
        with self.get_connection() as conn:
            # Convertir IDs a nombres usando tabla maestros
            palanca_query = "SELECT nombre FROM maestros WHERE tipo='palanca' AND codigo=?"
            kpi_query = "SELECT nombre FROM maestros WHERE tipo='kpi' AND codigo=?"
            
            palanca_name = conn.execute(palanca_query, [palanca_id]).fetchone()
            kpi_name = conn.execute(kpi_query, [kpi_id]).fetchone()
            
            if not palanca_name or not kpi_name:
                return []
            
            query = """
                SELECT period, test_value, control_value,
                       (test_value - COALESCE(control_value, 0)) as difference
                FROM experimentos 
                WHERE palanca = ? AND kpi = ?
                  AND ($3 IS NULL OR tipologia = $3)
                  AND period IS NOT NULL
                ORDER BY period
            """
            return [dict(row) for row in conn.execute(query, [palanca_name[0], kpi_name[0], tipologia])]
    
    # M√©todos para Chatbot (compatibilidad con sql_engine actual)
    def execute_query(self, sql_query: str) -> Dict[str, Any]:
        """Ejecuta consulta SQL para el chatbot (text-to-SQL)"""
        try:
            with self.get_connection() as conn:
                df = pd.read_sql_query(sql_query, conn)
                return {
                    'success': True,
                    'data': df.to_dict('records'),
                    'columns': df.columns.tolist(),
                    'row_count': len(df)
                }
        except Exception as e:
            self.logger.error(f"SQL execution failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'query': sql_query
            }
    
    def get_schema_info(self) -> str:
        """Informaci√≥n del schema para el LLM (compatible con sql_engine actual)"""
        return """
ESQUEMA DE BASE DE DATOS DISPONIBLE:

TABLA: pdvs
- codigo (TEXT): C√≥digo √∫nico del PDV (ej: PDV001, PDV002...)
- nombre (TEXT): Nombre del punto de venta
- ciudad (TEXT): Ciudad donde est√° ubicado
- region (TEXT): Regi√≥n geogr√°fica
- tipo (TEXT): Tipo de tienda

TABLA: metricas_revenue
- pdv_codigo (TEXT): C√≥digo del PDV
- periodo (TEXT): Per√≠odo YYYY-MM
- revenue (REAL): Ingresos en pesos
- visitantes (INTEGER): N√∫mero de visitantes
- conversiones (INTEGER): N√∫mero de conversiones
- tasa_conversion (REAL): Tasa de conversi√≥n calculada (%)
- revenue_por_visitante (REAL): Revenue promedio por visitante

TABLA: experimentos
- source (TEXT): Fuente del experimento
- tipologia (TEXT): Tipo de tienda
- palanca (TEXT): Palanca del experimento
- kpi (TEXT): KPI medido
- variacion_promedio (REAL): Variaci√≥n promedio
- diferencia_vs_control (REAL): Diferencia vs control

VISTA: vista_completa
- Combina pdvs y metricas_revenue con todos los campos

SIN√ìNIMOS RECONOCIDOS:
- revenue = ingreso = ingresos = facturaci√≥n
- conversiones = compras = transacciones
- visitantes = visitas = tr√°fico = clientes
"""

# Instancia global
unified_db = UnifiedDatabaseService()
```

#### **2.2 Migrar Endpoints del Dashboard**
```python
# Archivo: backend/app/routes/dashboard_unified.py

from flask import Blueprint, request, jsonify
from app.services.database_service import unified_db

dashboard_bp = Blueprint('dashboard_unified', __name__)

@dashboard_bp.route('/api/dashboard/results', methods=['GET'])
def get_dashboard_results():
    """Endpoint unificado para datos de dashboard"""
    try:
        tipologia = request.args.get('tipologia')
        data = unified_db.get_dashboard_results(tipologia)
        
        # Extraer palancas y KPIs √∫nicos
        palancas = list(set(row['palanca'] for row in data))
        kpis = list(set(row['kpi'] for row in data))
        
        return jsonify({
            'success': True,
            'data': data,
            'palancas': sorted(palancas),
            'kpis': sorted(kpis),
            'filtered_by': tipologia
        })
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@dashboard_bp.route('/api/dashboard/evolution-data', methods=['GET'])
def get_evolution_data():
    """Endpoint unificado para timeline chart"""
    try:
        palanca_id = int(request.args.get('palanca', 5))
        kpi_id = int(request.args.get('kpi', 1))
        tipologia = request.args.get('tipologia')
        
        data = unified_db.get_evolution_data(palanca_id, kpi_id, tipologia)
        
        return jsonify({
            'success': True,
            'data': data,
            'palanca_id': palanca_id,
            'kpi_id': kpi_id,
            'filtered_by': tipologia
        })
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
```

#### **2.3 Actualizar Chatbot para usar DB Unificada**
```python
# Modificaciones en: backend/app/chatbot.py

# Cambiar:
from app.data_store import DataStore
from app.sql_engine import SQLEngine

# Por:
from app.services.database_service import unified_db

class ABTestingChatbot:
    def __init__(self):
        Config.validate()
        self.client = OpenAI()
        self.memory = ConversationMemory()
        # Usar servicio unificado en lugar de DataStore + SQLEngine separados
        self.db_service = unified_db
    
    def get_response(self, user_message: str) -> str:
        """M√©todo actual sin cambios significativos"""
        # ... c√≥digo existente ...
        
        # Cambiar sql_engine por db_service:
        schema_info = self.db_service.get_schema_info()
        # ... resto del c√≥digo igual ...
    
    def _execute_sql_from_response(self, response_text, user_message: str):
        """Usar servicio unificado en lugar de sql_engine"""
        # ... extraer SQL igual que antes ...
        
        # Cambiar:
        result = self.sql_engine.execute_query(sql_query)
        
        # Por:
        result = self.db_service.execute_query(sql_query)
        
        # ... resto del c√≥digo igual ...
```

### **FASE 3: Migraci√≥n de Datos (1 d√≠a)**

#### **3.1 Script de Migraci√≥n**
```python
# Archivo: backend/scripts/migrate_data.py

import sqlite3
import requests
import json
from app.services.database_service import unified_db

def migrate_dashboard_data():
    """Migra datos del dashboard actual a la nueva DB"""
    
    # Obtener datos actuales via API
    current_data = requests.get('http://localhost:5000/api/dashboard/results').json()
    
    with unified_db.get_connection() as conn:
        for item in current_data['data']:
            conn.execute("""
                INSERT OR REPLACE INTO experimentos 
                (source, tipologia, palanca, kpi, variacion_promedio, diferencia_vs_control)
                VALUES (?, ?, ?, ?, ?, ?)
            """, [
                item['source'],
                'Super e Hiper',  # Valor por defecto
                item['palanca'],
                item['kpi'],
                item['variacion_promedio'],
                item['diferencia_vs_control']
            ])
        conn.commit()

def populate_sample_pdv_data():
    """Puebla datos de PDVs reales en lugar de simulados"""
    pdvs_data = [
        {'codigo': 'PDV001', 'nombre': 'Sucursal Centro', 'ciudad': 'Buenos Aires', 'region': 'Norte', 'tipo': 'Flagship'},
        {'codigo': 'PDV002', 'nombre': 'Local Palermo', 'ciudad': 'Buenos Aires', 'region': 'Norte', 'tipo': 'Standard'},
        # ... m√°s PDVs reales
    ]
    
    with unified_db.get_connection() as conn:
        for pdv in pdvs_data:
            conn.execute("""
                INSERT OR REPLACE INTO pdvs (codigo, nombre, ciudad, region, tipo)
                VALUES (?, ?, ?, ?, ?)
            """, [pdv['codigo'], pdv['nombre'], pdv['ciudad'], pdv['region'], pdv['tipo']])
        conn.commit()

if __name__ == '__main__':
    migrate_dashboard_data()
    populate_sample_pdv_data()
    print("‚úÖ Migraci√≥n completada")
```

### **FASE 4: Frontend - Sin Cambios Necesarios (0.5 d√≠as)**

El frontend **no requiere cambios** ya que:
- Los endpoints mantienen la misma interfaz
- Las respuestas JSON tienen el mismo formato
- Solo cambia la fuente de datos en el backend

**Verificaci√≥n requerida:**
```typescript
// Verificar que estos endpoints sigan funcionando:
apiService.getDashboardResults()
apiService.getEvolutionData()
apiService.sendMessage() // Para chatbot
```

### **FASE 5: Testing y Validaci√≥n (1-2 d√≠as)**

#### **5.1 Tests Automatizados**
```python
# Archivo: backend/tests/test_unified_database.py

import unittest
from app.services.database_service import unified_db

class TestUnifiedDatabase(unittest.TestCase):
    
    def test_dashboard_results_consistency(self):
        """Verifica que dashboard devuelve datos consistentes"""
        results = unified_db.get_dashboard_results()
        self.assertGreater(len(results), 0)
        
        for result in results:
            self.assertIn('source', result)
            self.assertIn('palanca', result)
            self.assertIn('kpi', result)
    
    def test_chatbot_sql_execution(self):
        """Verifica que chatbot puede ejecutar SQL correctamente"""
        result = unified_db.execute_query("SELECT COUNT(*) as total FROM pdvs")
        self.assertTrue(result['success'])
        self.assertGreater(result['data'][0]['total'], 0)
    
    def test_schema_consistency(self):
        """Verifica que el schema incluye todas las tablas necesarias"""
        schema_info = unified_db.get_schema_info()
        self.assertIn('pdvs', schema_info)
        self.assertIn('experimentos', schema_info)
        self.assertIn('vista_completa', schema_info)

if __name__ == '__main__':
    unittest.main()
```

#### **5.2 Tests de Integraci√≥n**
```bash
# Verificar endpoints funcionando
curl http://localhost:5000/api/dashboard/results
curl http://localhost:5000/api/chat/start -X POST -d '{"userEmail":"test@test.com"}'

# Verificar frontend carga correctamente
npm run dev # Verificar dashboard muestra datos
```

### **FASE 6: Deploy y Monitoreo (0.5 d√≠as)**

#### **6.1 Actualizar Docker Compose**
```yaml
# docker-compose.yml
version: '3.8'
services:
  backend:
    # ... configuraci√≥n existente ...
    volumes:
      - ./data:/app/data  # Persistir SQLite
    environment:
      - DATABASE_PATH=/app/data/unified.db
```

#### **6.2 Backup Strategy**
```bash
# Script de backup autom√°tico
#!/bin/bash
# backup-db.sh
cp data/unified.db "backups/unified_$(date +%Y%m%d_%H%M%S).db"
```

---

## üêò OPCI√ìN 2: Migraci√≥n a PostgreSQL

### **Cu√°ndo Elegir PostgreSQL**
- üìà Dataset > 100MB o crecimiento proyectado
- üë• M√∫ltiples usuarios concurrentes (>50)
- üåê Deploy multi-instancia/escalabilidad
- üîê Requerimientos de seguridad avanzados
- üìä Necesidad de funciones SQL avanzadas

### **Cambios Adicionales vs SQLite**

#### **Dependencias**
```python
# requirements.txt - a√±adir:
psycopg2-binary==2.9.7
SQLAlchemy==2.0.21  # ORM recomendado para PostgreSQL
```

#### **Configuraci√≥n**
```python
# backend/app/config.py - a√±adir:
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://user:pass@localhost:5432/dbname')
```

#### **Servicio de Base de Datos**
```python
# backend/app/services/database_service.py
import psycopg2
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

class UnifiedDatabaseService:
    def __init__(self):
        self.engine = create_engine(Config.DATABASE_URL)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def execute_query(self, sql_query: str):
        """PostgreSQL version con mejor manejo de concurrencia"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(sql_query))
                return {
                    'success': True,
                    'data': [dict(row) for row in result],
                    'row_count': result.rowcount
                }
        except Exception as e:
            return {'success': False, 'error': str(e)}
```

#### **Docker Compose con PostgreSQL**
```yaml
version: '3.8'
services:
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: gatorade_ab
      POSTGRES_USER: gatorade
      POSTGRES_PASSWORD: secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema.sql:/docker-entrypoint-initdb.d/schema.sql
    ports:
      - "5432:5432"

  backend:
    # ... resto de configuraci√≥n ...
    depends_on:
      - db
    environment:
      - DATABASE_URL=postgresql://gatorade:secure_password@db:5432/gatorade_ab

volumes:
  postgres_data:
```

---

## üìä Comparaci√≥n Final

| Aspecto | SQLite | PostgreSQL |
|---------|--------|------------|
| **Complejidad Setup** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Simple | ‚≠ê‚≠ê‚≠ê Media |
| **Performance (Dataset Actual)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê Muy Bueno |
| **Escalabilidad** | ‚≠ê‚≠ê‚≠ê Limitada | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente |
| **Mantenimiento** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M√≠nimo | ‚≠ê‚≠ê‚≠ê Medio |
| **Deploy Complexity** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Trivial | ‚≠ê‚≠ê Complejo |
| **Backup/Restore** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Copy File | ‚≠ê‚≠ê‚≠ê Scripts |
| **Costo Infraestructura** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Cero | ‚≠ê‚≠ê‚≠ê Medio |

---

## üéØ DECISI√ìN FINAL: PostgreSQL ‚úÖ

**DECISI√ìN EJECUTIVA (Octubre 2025): Migrar a PostgreSQL**

### **Razones para PostgreSQL sobre SQLite:**

1. **Crecimiento de datos A/B testing** ‚úÖ
   - Tabla `ab_test_result` crece continuamente (50+ stores √ó 10+ categor√≠as √ó 52 semanas)
   - Proyecci√≥n: 150K-250K registros en 3-5 a√±os
   - PostgreSQL maneja esto sin esfuerzo

2. **Concurrencia real Dashboard + Chatbot** ‚úÖ
   - Usuarios simult√°neos consultando dashboard
   - Sesiones de chatbot ejecutando text-to-SQL en paralelo
   - SQLite bloquea en escrituras, PostgreSQL usa MVCC

3. **Capacidades anal√≠ticas avanzadas** ‚úÖ
   - Window functions (LAG, LEAD) para an√°lisis de tendencias
   - CTEs complejos para consultas del chatbot
   - JSON aggregations para APIs
   - Better SQL = Better text-to-SQL

4. **Cloud-ready y escalabilidad** ‚úÖ
   - Integraci√≥n nativa con Cloud Run SQL / Render / Supabase
   - Read replicas para escalar lecturas
   - Backup online sin downtime
   - M√∫ltiples instancias backend sin problemas

5. **Text-to-SQL igual de efectivo** ‚úÖ
   - LLMs (GPT-4, Claude) entrenan con PostgreSQL syntax
   - Mejores mensajes de error para debugging
   - Sintaxis est√°ndar vs SQLite quirks

### **Trade-off Aceptado:**
- ‚è±Ô∏è +1.5 d√≠as de desarrollo vs SQLite
- üîß Configuraci√≥n inicial m√°s compleja
- üí™ **Beneficio:** Base s√≥lida para escalar 3-5 a√±os

### **Esquema Real a Migrar:**

**Fuente:** `app_db_20251008_2014.xlsx` (cada hoja = tabla)

```
Tablas Maestras:
- city_master (ciudades)
- store_master (tiendas/PDVs con c√≥digos sell-in/sell-out)
- typology_master (Super & Hyper, Convenience, Pharmacies)
- lever_master (palancas: Square meters, Checkout cooler, etc.)
- category_master (Gatorade, 500ml, 1000ml, Sugar-free)
- measurement_unit_master (Standardized Cases, Sales)
- data_source_master (Sell In, Sell Out)
- period_master (per√≠odos semanales/mensuales)

Tablas de Hechos:
- ab_test_result (resultados detallados por tienda/per√≠odo)
- ab_test_summary (res√∫menes agregados por tipolog√≠a/palanca)
```

---

## üìÖ Timeline PostgreSQL - ACTUALIZADO (Octubre 2025)

### **Progreso Actual:**

| Fase | Estimado | Estado | Archivos Creados |
|------|----------|--------|------------------|
| **‚úÖ Fase 1: Preparaci√≥n** | 2-3 d√≠as | **COMPLETADO** | ‚úì Schema PostgreSQL<br>‚úì Docker Compose<br>‚úì Script migraci√≥n<br>‚úì UnifiedDatabaseService |
| **üîÑ Fase 2: Backend Changes** | 3-4 d√≠as | **50% COMPLETO** | ‚è≥ Migrar endpoints<br>‚è≥ Actualizar chatbot |
| **‚è≥ Fase 3: Data Migration** | 1-2 d√≠as | PENDIENTE | - |
| **‚è≥ Fase 4: Frontend** | 0.5 d√≠as | PENDIENTE | - |
| **‚è≥ Fase 5: Testing** | 2-3 d√≠as | PENDIENTE | - |
| **‚è≥ Fase 6: Deploy** | 1-2 d√≠as | PENDIENTE | - |
| **TOTAL** | **8-14 d√≠as** | **~7-10 d√≠as restantes** | - |

---

## üìÅ Archivos Creados Hoy (Sesi√≥n 1)

### **1. Schema PostgreSQL** ‚úÖ
**Archivo:** `backend/database/schema.sql`
- 10 tablas maestras + 2 tablas de hechos
- 3 vistas SQL para chatbot (`v_chatbot_complete`, `v_dashboard_summary`, `v_evolution_timeline`)
- √çndices para performance
- Triggers `updated_at` autom√°ticos
- Comentarios y documentaci√≥n

### **2. Docker Compose PostgreSQL** ‚úÖ
**Archivo:** `docker-compose.postgres.yml`
- PostgreSQL 15-alpine
- Backend con `DATABASE_URL`
- Frontend sin cambios
- pgAdmin opcional (profile: tools)
- Volumes persistentes
- Health checks configurados

### **3. Script de Migraci√≥n** ‚úÖ
**Archivo:** `backend/scripts/migrate_excel_to_postgres.py`
- Lee `app_db_20251008_2014.xlsx`
- Migra todas las hojas a PostgreSQL
- Respeta orden de dependencias (maestros ‚Üí hechos)
- Flags: `--truncate`, `--validate-only`
- Batch insert con `execute_values`

### **4. Unified Database Service** ‚úÖ
**Archivo:** `backend/app/services/unified_database_service.py`
- SQLAlchemy + Connection Pooling
- M√©todos Dashboard: `get_dashboard_results()`, `get_evolution_data()`, `get_filter_options()`
- M√©todos Chatbot: `execute_query()`, `get_schema_info()`
- Compatible con APIs existentes
- Schema info completo para text-to-SQL

### **5. Dependencias Actualizadas** ‚úÖ
**Archivo:** `backend/requirements.txt`
- `psycopg2-binary==2.9.9`
- `SQLAlchemy==2.0.25`
- `alembic==1.13.1`

### **6. Variables de Entorno** ‚úÖ
**Archivo:** `.env.example`
- `DB_PASSWORD`
- `DATABASE_URL`
- `PGADMIN_PASSWORD`

---

## üöÄ PR√ìXIMOS PASOS (Para Ma√±ana)

### **PASO 1: Levantar PostgreSQL y Migrar Datos (1 hora)**

```bash
# 1. Copiar variables de entorno
cp .env.example .env
# Editar .env y agregar OPENAI_API_KEY

# 2. Levantar PostgreSQL
docker-compose -f docker-compose.postgres.yml up -d db

# 3. Verificar que PostgreSQL est√° corriendo
docker-compose -f docker-compose.postgres.yml ps
docker-compose -f docker-compose.postgres.yml logs db

# 4. Ejecutar migraci√≥n de datos
python backend/scripts/migrate_excel_to_postgres.py --truncate

# 5. Validar migraci√≥n
python backend/scripts/migrate_excel_to_postgres.py --validate-only
```

### **PASO 2: Migrar Endpoints del Dashboard (2 horas)**

**Archivos a modificar:**
1. `backend/app/routes/analytics.py`
   - Reemplazar `excel_service` por `unified_db`
   - Actualizar `/api/dashboard/results`
   - Actualizar `/api/dashboard/evolution-data`
   - Actualizar `/api/dashboard/filter-options`

2. `backend/app/__init__.py`
   - Importar `unified_db`
   - Agregar health check de PostgreSQL

**C√≥digo de ejemplo:**
```python
# En analytics.py
from app.services.unified_database_service import unified_db

@analytics_bp.route('/api/dashboard/results', methods=['GET'])
def get_dashboard_results():
    tipologia = request.args.get('tipologia')
    result = unified_db.get_dashboard_results(tipologia)
    return jsonify(result), 200
```

### **PASO 3: Migrar Chatbot a PostgreSQL (2 horas)**

**Archivos a modificar:**
1. `backend/app/chatbot.py`
   ```python
   # Cambiar:
   from app.data_store import DataStore
   from app.sql_engine import SQLEngine

   # Por:
   from app.services.unified_database_service import unified_db

   class ABTestingChatbot:
       def __init__(self):
           # ...
           self.db_service = unified_db  # En lugar de sql_engine
   ```

2. `backend/app/services/chatbot_service.py`
   - Actualizar referencias a `sql_engine`
   - Usar `unified_db.execute_query()` y `unified_db.get_schema_info()`

### **PASO 4: Testing Integrado (1 hora)**

```bash
# 1. Levantar todos los servicios
docker-compose -f docker-compose.postgres.yml up -d

# 2. Test endpoints dashboard
curl http://localhost:5000/api/dashboard/results
curl http://localhost:5000/api/dashboard/filter-options

# 3. Test chatbot
curl -X POST http://localhost:5000/api/chat/start \
  -H "Content-Type: application/json" \
  -d '{"userEmail":"test@test.com"}'

# 4. Verificar frontend
# Abrir: http://localhost:5173
```

### **PASO 5: Crear Tests Automatizados (2 horas)**

**Archivo:** `backend/tests/test_unified_database.py`
```python
import unittest
from app.services.unified_database_service import unified_db

class TestUnifiedDatabase(unittest.TestCase):
    def test_dashboard_results(self):
        result = unified_db.get_dashboard_results()
        self.assertTrue(result['success'])
        self.assertGreater(len(result['data']), 0)

    def test_chatbot_query(self):
        result = unified_db.execute_query(
            "SELECT COUNT(*) as total FROM store_master"
        )
        self.assertTrue(result['success'])
```

---

## ‚ö†Ô∏è DECISIONES T√âCNICAS TOMADAS

### **1. Vistas SQL para Chatbot**
- **Decisi√≥n:** Crear vistas (`v_chatbot_complete`, etc.) en lugar de hacer joins en Python
- **Raz√≥n:** El LLM puede consultar vistas directamente, simplifica text-to-SQL
- **Trade-off:** M√°s complejidad en schema, pero mejor performance

### **2. IDs vs Nombres en Maestros**
- **Decisi√≥n:** Usar IDs como PKs, nombres como √∫nicos
- **Raz√≥n:** Normalizaci√≥n est√°ndar, permite cambios de nombre sin romper FKs
- **Frontend:** Debe convertir nombres a IDs antes de consultar

### **3. Compatible con APIs Existentes**
- **Decisi√≥n:** `UnifiedDatabaseService` retorna mismo formato que `excel_service`
- **Raz√≥n:** Frontend NO requiere cambios
- **Ventaja:** Migraci√≥n transparente

### **4. Connection Pooling**
- **Decisi√≥n:** SQLAlchemy con pool_size=10, max_overflow=20
- **Raz√≥n:** Dashboard + Chatbot concurrentes requieren m√∫ltiples conexiones
- **Configuraci√≥n:** Ajustable en `unified_database_service.py`

---

## üîß Comandos √ötiles

### **Docker PostgreSQL:**
```bash
# Levantar solo PostgreSQL
docker-compose -f docker-compose.postgres.yml up -d db

# Ver logs de PostgreSQL
docker-compose -f docker-compose.postgres.yml logs -f db

# Conectar a PostgreSQL CLI
docker exec -it gatorade_postgres psql -U gatorade_user -d gatorade_ab_testing

# Levantar con pgAdmin
docker-compose -f docker-compose.postgres.yml --profile tools up -d

# Acceder pgAdmin: http://localhost:5050
# Email: admin@gatorade.com, Password: admin
```

### **Backup y Restore:**
```bash
# Backup
docker exec gatorade_postgres pg_dump -U gatorade_user gatorade_ab_testing > backup.sql

# Restore
docker exec -i gatorade_postgres psql -U gatorade_user gatorade_ab_testing < backup.sql
```

---

## ‚úÖ Criterios de √âxito - TODOS COMPLETADOS

- [x] PostgreSQL corriendo en Docker ‚úÖ
- [x] Datos migrados desde Excel ‚úÖ (38,470 registros)
- [x] Dashboard consume PostgreSQL ‚úÖ (analytics.py migrado)
- [x] Chatbot consume PostgreSQL ‚úÖ (chatbot.py migrado)
- [x] Datos consistentes Dashboard-Chatbot ‚úÖ (misma fuente: unified_db)
- [x] Performance >= Excel actual ‚úÖ (probado con curl)
- [x] Tests integrados completos ‚úÖ (endpoints y chatbot probados)
- [x] Documentaci√≥n actualizada ‚úÖ (CLAUDE.md y NEXT_STEPS.md)

---

## üìù Notas para Continuar

**Archivos que NO se deben modificar (a√∫n):**
- `frontend/**/*` - Frontend mantiene compatibilidad
- `backend/app/data_store.py` - Se deprecar√° despu√©s
- `backend/app/sql_engine.py` - Se deprecar√° despu√©s
- `backend/app/services/excel_service.py` - Se deprecar√° despu√©s

**Orden de migraci√≥n recomendado:**
1. ‚úÖ Schema + Docker + Scripts (COMPLETADO)
2. ‚è≥ Dashboard endpoints ‚Üí PostgreSQL (SIGUIENTE)
3. ‚è≥ Chatbot ‚Üí PostgreSQL
4. ‚è≥ Tests integraci√≥n
5. ‚è≥ Deprecar archivos antiguos
6. ‚è≥ Deploy producci√≥n

**Tiempo estimado restante:** 7-10 d√≠as (distribuidos en bloques de 2-4 horas/d√≠a)